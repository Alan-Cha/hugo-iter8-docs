[{"uri":"https://alan-cha.github.io/hugo-iter8-docs/tutorials/canary/","title":"Getting started with canary testing","tags":[],"description":"","content":"This tutorial shows how iter8 can be used to perform a canary release by gradually shifting traffic from one version of a microservice to another while evaluating the behavior of the new version. Traffic is fully shifted only if the behavior the candidate version meets specified acceptance criteria.\nThis tutorial has seven steps, which are meant to be tried in order. You will learn:\n how to perform a canary rollout with iter8; and how to define different success criteria for iter8 to analyze canary releases and determine success or failure;  The tutorial is based on the Bookinfo sample application distributed with Istio. This application comprises 4 microservies: productpage, details, reviews, and ratings. Of these, productpage is a user-facing service while the others are backend services.\nThis tutorial assumes you have already installed iter8 (including Istio). If not, do so using the instructions here.\nDeploy the Bookinfo application To deploy the Bookinfo application, create a namespace configured to enable auto-injection of the Istio sidecar. You can use whatever namespace name you wish. By default, the namespace bookinfo-iter8 is created.\nexport NAMESPACE=bookinfo-iter8 curl -s https://raw.githubusercontent.com/Alan-Cha/hugo-iter8-docs/master/static/tutorials/namespace.yaml \\  | sed \u0026#34;s#bookinfo-iter8#$NAMESPACE#\u0026#34; \\  | kubectl apply -f - Next, deploy the application:\nkubectl --namespace $NAMESPACE apply -f https://raw.githubusercontent.com/Alan-Cha/hugo-iter8-docs/master/static/tutorials/bookinfo-tutorial.yaml You should see pods for each of the four microservices:\nkubectl --namespace $NAMESPACE get pods Note that we deployed version v2 of the reviews microsevice; that is, reviews-v2. Each pod should have two containers, since the Istio sidecar was injected into each.\nExpose the Bookinfo application Expose the Bookinfo application by defining an Istio Gateway and VirtualService:\nkubectl --namespace $NAMESPACE apply -f https://raw.githubusercontent.com/Alan-Cha/hugo-iter8-docs/master/static/tutorials/bookinfo-gateway.yaml You can inspect the created resources:\nkubectl --namespace $NAMESPACE get gateway,virtualservice Note that the service has been associated with a fake host, bookinfo.example.com for demonstration purposes.\nVerify access to Bookinfo To access the application, determine the ingress IP and port for the application. You can do so by following steps 3 and 4 of the Istio instructions here to set the environment variables GATEWAY_URL. You can then check if you can access the application with the following curl command:\ncurl --header \u0026#39;Host: bookinfo.example.com\u0026#39; -o /dev/null -s -w \u0026#34;%{http_code}\\n\u0026#34; \u0026#34;http://${GATEWAY_URL}/productpage\u0026#34; If everything is working, the command above should return 200. Note that the curl command above sets the Host header to match the host we associated the VirtualService with (bookinfo.example.com).\nNote: If you want to access the application from your browser, you will need to set this header using a browser plugin.\nGenerate load To simulate user requests, use a command such as the following:\nwatch -n 0.1 \u0026#39;curl --header \u0026#34;Host: bookinfo.example.com\u0026#34; -s \u0026#34;http://${GATEWAY_URL}/productpage\u0026#34; | grep -i \u0026#34;color=\\\u0026#34;\u0026#34;\u0026#39; This command requests the productpage microservice 10 times per second. In turn, this causes about the same frequency of requests against the backend microservice. We filter the response to see the color being used to display the \u0026ldquo;star\u0026rdquo; rating of the application. The color varies between versions giving us a visual way to distinguish between them.\nCreate a canary Experiment We will now define a canary experiment to rollout version v3 of the reviews application. These versions are identical except for the color of the stars that appear on the page. In version v3 they are red. This can be seen in the inspected in the output of the above watch command. As version v3 is rolled out, you should see the color change.\nTo describe a canary rollout, create an iter8 Experiment that identifies the original, or baseline version and the new, or candidate version and some evaluation criteria. For example:\napiVersion: iter8.tools/v1alpha2 kind: Experiment metadata: name: reviews-v3-rollout spec: service: name: reviews baseline: reviews-v2 candidates: [ \u0026#34;reviews-v3\u0026#34; ] criteria: - metric: iter8_mean_latency threshold: type: absolute value: 200 duration: maxIterations: 8 interval: 15s trafficControl: maxIncrement: 20 In this example, the target of the experiment is the service reviews. The baseline and candidate versions are specified using their Deployment names, reviews_v2 and reviews_v3, respectively. A single evaluation criteria is specified. It requires that the measurements of the metric iter8_mean_latency should all return values less than 200 milliseconds. The additional parameters control how long the experiment should run and how much traffic can be shifted to the new version in each interval. Details regarding these parameters are here.\nThe experiment can be created using the command:\nkubectl --namespace $NAMESPACE apply -f https://raw.githubusercontent.com/Alan-Cha/hugo-iter8-docs/master/static/tutorials/canary_reviews-v2_to_reviews-v3.yaml Inspection of the new experiment shows that it is paused because the specified candidate version cannot be found in the cluster:\nkubectl --namespace $NAMESPACE get experiment NAME TYPE HOSTS PHASE WINNER FOUND CURRENT BEST STATUS reviews-v3-rollout Canary [reviews] Pause TargetsError: Missing Candidate Once the candidate version is deployed, the experiment will start automatically.\nDeploy the candidate version of the reviews service To deploy version v3 of the reviews microservice, execute:\nkubectl --namespace $NAMESPACE apply -f https://raw.githubusercontent.com/Alan-Cha/hugo-iter8-docs/master/static/tutorials/reviews-v3.yaml Once its corresponding pods have started, the Experiment will show that it is progressing:\nkubectl --namespace $NAMESPACE get experiment NAME TYPE HOSTS PHASE WINNER FOUND CURRENT BEST STATUS reviews-v3-rollout Canary [reviews] Progressing false reviews-v3 IterationUpdate: Iteration 0/8 completed At approximately 15 second intervals, you should see the interation number change. Traffic will gradually be shifted (in 20% increments) from version v2 to version v3. iter8 will quickly identify that the best version is the candidate, reviews-v3 and that it is confident that this choice will be the final choice (by indicating that a winner has been found:\nkubectl --namespace $NAMESPACE get experiment NAME TYPE HOSTS PHASE WINNER FOUND CURRENT BEST STATUS reviews-v3-rollout Canary [reviews] Progressing true reviews-v2 IterationUpdate: Iteration 3/8 completed When the experiment is finished (about 2 minutes), you will see that all traffic has been shifted to the winner, reviews-v3:\nkubectl --namespace $NAMESPACE get experiment NAME TYPE HOSTS PHASE WINNER FOUND CURRENT BEST STATUS reviews-v3-rollout Canary [reviews] Completed true reviews-v3 ExperimentCompleted: Traffic To Winner Cleanup To clean up, delete the namespace:\nkubectl delete namespace $NAMESPACE Other things to try (before cleanup) Inspect progress using Grafana Coming soon\nInspect progress using Kiali Coming soon\nAlter the duration of the experiment The progress of an experiment can be impacted by duration and trafficControl parameters:\n duration.interval defines how long each test interval should be (default: 30 seconds) duration.maxIterations identifies what the maximum number of iterations there should be (default: 100) trafficControl.maxIncrement identifies the largest change (increment) that will be made in the percentage of traffic sent to a candidate (default: 2 percent)  The impact of the first two parameters on the duration of the experiment are clear. Restricting the size of traffic shifts limits how quickly an experiment can come to a decision about a candidate.\nTry a version that fails the criteria Version v4 of the reviews service is a modification that returns after a 5 second delay. If you try this version as a candidate, you should see the canary experiment reject it and choose the baseline version as the winner.\nFor your reference:\n A YAML for the deployment reviews-v4 is: https://raw.githubusercontent.com/Alan-Cha/hugo-iter8-docs/master/static/tutorials/reviews-v4.yaml A YAML for an canary experiment from reviews-v3 to reviews-v4 is: https://raw.githubusercontent.com/Alan-Cha/hugo-iter8-docs/master/static/tutorials/canary_reviews-v3_to_reviews-v4.yaml  Try a version which returns errors Coming soon\nTry with a user-facing service Coming soon\n"},{"uri":"https://alan-cha.github.io/hugo-iter8-docs/","title":"Homepage","tags":[],"description":"","content":"Deliver better software in the cloud Use iter8\u0026rsquo;s analytics-driven continuous experimentation for reliable and frequent releases of high-quality microservices on Kubernetes.\n Automate canary releases Use advanced statistical algorithms to assess key metrics for your service and progressively shift traffic to the winning release.\n Launch experiments rapidly With iter8\u0026rsquo;s Kiali UI, you can create and launch canary release experiments for your service in seconds, and observe and control these experiments in real-time.\n Analyze long-term trends Analyze how key metrics for your service have evolved over multiple releases using iter8-trend and Grafana.\n Explore iter8\n"},{"uri":"https://alan-cha.github.io/hugo-iter8-docs/introduction/","title":"Introduction","tags":[],"description":"","content":"Introduction  Iter8 enables statistically robust continuous experimentation of microservices in your CI/CD pipelines  Learn more about iter8\n   Installation  Installation Iter8 on Kubernetes and Istio Learn how to install iter8 on Kubernetes and Istio Iter8 on Red Hat OpenShift Learn how to install iter8 on Red Hat OpenShift   Iter8 on Kubernetes and Istio  Learn how to install iter8 on Kubernetes and Istio\n   Iter8 on Red Hat OpenShift  Learn how to install iter8 on Red Hat OpenShift\n    "},{"uri":"https://alan-cha.github.io/hugo-iter8-docs/introduction/about/","title":"Iter8 enables statistically robust continuous experimentation of microservices in your CI/CD pipelines","tags":[],"description":"","content":"Use an iter8 experiment to safely expose competing versions of a service to application traffic, gather in-depth insights about key performance and business metrics for your microservice versions, and intelligently rollout the best version of your service.\nIter8\u0026rsquo;s expressive model of cloud experimentation supports a variety of CI/CD scenarios. Using an iter8 experiment, you can:\n Run a performance test with a single version of a microservice. Perform a canary release with two versions, a baseline and a candidate. Iter8 will shift application traffic safely and gradually to the candidate, if it meets the criteria you specify in the experiment. Perform an A/B test with two versions \u0026ndash; a baseline and a candidate. Iter8 will identify and shift application traffic safely and gradually to the winner, where the winning version is defined by the criteria you specify in the experiment. Perform an A/B/N test with multiple versions \u0026ndash; a baseline and multiple candidates. Iter8 will identify and shift application traffic safely and gradually to the winner.  Under the hood, iter8 uses advanced Bayesian learning techniques coupled with multi-armed bandit approaches to compute a variety of statistical assessments for your microservice versions, and uses them to make robust traffic control and rollout decisions.\n"},{"uri":"https://alan-cha.github.io/hugo-iter8-docs/introduction/installation/kubernetes/","title":"Iter8 on Kubernetes and Istio","tags":[],"description":"","content":"These instructions show you how to set up iter8 on Kubernetes with Istio.\nPrerequisites  Kubernetes v1.11 or newer. Istio v1.1.5 and newer. Your Istio installation must have at least the istio-pilot as well as telemetry and Prometheus enabled.  Install iter8 on Kubernetes iter8 has two components, iter8_analytics and iter8_controller. To install them, follow the instructions below. For additional considerations when installing iter8 on Red Hat OpenShift, check out these instructions.\nQuick installation To install iter8 with the default settings, you can run the following install script:\ncurl -L -s https://raw.githubusercontent.com/iter8-tools/iter8-controller/v0.2.1/install/install.sh \\ | /bin/bash - Customized installation via Helm charts In case you need to customize the installation of iter8, use the Helm charts listed below:\n  iter8-analytics: https://github.com/iter8-tools/iter8-analytics/releases/download/v0.2.1/iter8-analytics-helm-chart.tar\n  iter8-controller: https://github.com/iter8-tools/iter8-controller/releases/download/v0.2.1/iter8-controller-helm-chart.tar\n  Note on Prometheus: In order to make assessments, iter8-analytics needs to query metrics collected by Istio and stored on Prometheus. The default values for the helm chart parameters (used in the quick installation) point iter8-analytics to the Prometheus server at http://prometheus.istio-system:9090 (the default internal Kubernetes URL of Prometheus installed as an Istio addon) without specifying any need for authentication. If your Istio installation is shipping metrics to a different Prometheus service, or if you need to configure authentication to access Prometheus, you need to set appropriate iter8-analytics Helm chart parameters. Look in the section metricsBackend of the Helm chart\u0026rsquo;s values.yaml file for details.\nNote on Istio Telemetry: When deploying iter8-controller using helm, make sure to set the parameter istioTelemetry to conform with your environment. Possible values are v1 or v2. Use v1 if the Istio mixer is not disabled. You can determine whether or not the mixer is disabled using this command:\nkubectl -n $ISTIO_NAMESPACE get cm istio -o json | jq .data.mesh | grep -o \u0026#39;disableMixerHttpReports: [A-Za-z]\\+\u0026#39; | cut -d \u0026#39; \u0026#39; -f2 Verify the installation After installing iter8-analytics and iter8-controller, you should see the following pods and services in the newly created iter8 namespace:\n$ kubectl get pods -n iter8 NAME READY STATUS RESTARTS AGE iter8-controller-5f54bb4b88-drr8s 1/1 Running 0 4s iter8-analytics-5c5758ccf9-p575b 1/1 Running 0 61s $ kubectl get svc -n iter8 NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE iter8-controller ClusterIP 172.21.62.217 \u0026lt;none\u0026gt; 443/TCP 20s iter8-analytics ClusterIP 172.21.106.44 \u0026lt;none\u0026gt; 80/TCP 76s Import iter8\u0026rsquo;s Grafana dashboard To enable users to see Prometheus metrics that pertain to their canary releases or A/B tests, iter8 provides a Grafana dashboard template. To take advantage of Grafana, you will need to import this template. To do so, first make sure you can access Grafana. In a typical Istio installation, you can port-forward Grafana from Kubernetes to your localhost\u0026rsquo;s port 3000 with the command below:\nkubectl -n istio-system port-forward $(kubectl -n istio-system get pod -l app=grafana -o jsonpath=\u0026#39;{.items[0].metadata.name}\u0026#39;) 3000:3000 After running that command, you can access Grafana\u0026rsquo;s UI at http://localhost:3000.Iter8 dashboard can be imported by:\ncurl -L -s https://raw.githubusercontent.com/iter8-tools/iter8-controller/v0.2.1/hack/grafana_install_dashboard.sh \\ | /bin/bash - Uninstall iter8 If you want to uninstall all iter8 components from your Kubernetes cluster, first delete all instances of Experiment from all namespaces. Then, you can delete iter8 by running the following command:\nkubectl delete -f https://raw.githubusercontent.com/iter8-tools/iter8-controller/v0.2.1/install/iter8-controller.yaml Note that this command will delete the Experiment CRD and wipe out the iter8 namespace, but it will not remove the iter8 Grafana dashboard if created.\n"},{"uri":"https://alan-cha.github.io/hugo-iter8-docs/introduction/installation/red-hat/","title":"Iter8 on Red Hat OpenShift","tags":[],"description":"","content":"These instructions show you how to set up iter8 on Red Hat OpenShift.\nPrerequisites We recommend using the Red Hat OpenShift Service Mesh. This can be installed using the Red Hat OpenShift Service Mesh Operator. For details, see: https://docs.openshift.com/container-platform/4.3/service_mesh/service_mesh_install/installing-ossm.html.\nInstalling the Service Mesh involves installing the Elasticsearch, Jaeger, Kiali and Red Hat OpenShift Service Mesh Operators, creating and managing a ServiceMeshControlPlane resource to deploy the control plane, and creating a ServiceMeshMemberRoll resource to specify the namespaces associated with the Red Hat OpenShift Service Mesh.\nInstalling iter8 By default, iter8 uses the Prometheus service installed as part of the Red Hat OpenShift Service Mesh for the metrics used to assess the quality of different versions of a service. The Red Hat OpenShift Service Mesh configures the Prometheus service to require authentication. To configure iter8 to authenticate with Prometheus, some additional steps are needed.\nInstall the iter8 analytics service Download and untar the helm chart for the iter8-analytics service. The following options can be used to generate the needed yaml:\nREPO=iter8/iter8-analytics PROMETHEUS_SERVICE=\u0026#39;https://prometheus.istio-system:9090\u0026#39; PROMETHEUS_USERNAME=\u0026#39;internal\u0026#39; PROMETHEUS_PASSWORD=\u0026lt;FILL IN\u0026gt; helm template install/kubernetes/helm/iter8-analytics \\  --name iter8-analytics \\  --set image.repository=${REPO} \\  --set image.tag=v0.2.1 \\  --set iter8Config.authentication.type=basic \\  --set iter8Config.authentication.username=${PROMETHEUS_USERNAME} \\  --set iter8Config.authentication.password=${PROMETHEUS_PASSWORD} \\  --set iter8Config.authentication.insecure_skip_verify=true \\  --set iter8Config.metricsBackendURL=${PROMETHEUS_SERVICE} \\ | kubectl -n iter8 apply -f - The password, to be used can be found in the secret htpasswd in the namespace where Istio is installed. For example, the following might work to identify it:\nPROMETHEUS_PASSWORD=$(kubectl -n istio-system get secret htpasswd -o jsonpath=\u0026#39;{.data.rawPassword}\u0026#39; | base64 --decode) Install the iter8 controller The quick install instructions can be used to install the iter8 controller. The Service Mesh currently uses Istio telemetry version v1:\nkubectl apply -f https://raw.githubusercontent.com/iter8-tools/iter8-controller/v0.2.1/install/iter8-controller.yaml Target Services The Red Hat OpenShift Service Mesh is restricted to the set of namespaces defined in the ServiceMeshMemberRoll resource. In particular, if you will be trying the tutorials, add the namespace bookinfo-iter8 to the ServiceMeshMemberRoll.\nIstio relies a sidecar injected into each pod to provide its capabilities. Istio provides several ways this sidecar can be injected. Red Hat recommends the use of the annotation sidecar.istio.io/inject: \u0026quot;true\u0026quot; in the deployment yaml. Examples can be found in the yaml for the tutorial: https://github.com/iter8-tools/iter8-controller/blob/v0.2.1/doc/tutorials/istio/bookinfo/bookinfo-tutorial.yaml\n"},{"uri":"https://alan-cha.github.io/hugo-iter8-docs/reference/metrics/","title":"Iter8&#39;s metrics","tags":[],"description":"","content":"This document describes iter8\u0026rsquo;s out-of-the-box metrics, the anatomy of a metric definition, and how users can define their own metrics.\nMetrics defined by iter8 By default, iter8 leverages the metrics collected by Istio telemetry and stored in Prometheus. Users relying on iter8\u0026rsquo;s out-of-the-box metrics can simply reference them in the success criteria of an experiment specification, as shown in the Experiment CRD documentation.\nDuring an experiment, for every call made from iter8-controller to iter8-analytics, the latter in turn calls Prometheus to retrieve values of the metrics referenced by the Kubernetes experiment resource. Iter8-analytics analyzes the service versions that are part of the experiment and arrives at an assessment based on their metric values. It returns this assessment to iter8-controller.\nIn particular, the following metrics are available out-of-the-box from iter8. These metrics are based on the telemetry data collected by Istio.\n  iter8_latency: mean latency, that is, average time taken by the service version to respond to HTTP requests.\n  iter8_error_count: total error count, that is, number of HTTP requests that resulted in error (5xx HTTP status codes).\n  iter8_error_rate: error rate, that is, (total error count / total number of HTTP requests).\n  When iter8 is installed, a Kubernetes ConfigMap named iter8config-metrics is populated with a definition for each of the above metrics. You can see the metric definitions in this file. A few things to note in the definitions:\n  Each metric is defined under the metrics section.\n  They refer back to a Prometheus query template defined under the query_templates section. Iter8 uses that template to query Prometheus and compute the value of the metric for every service version.\n  If this metric is a counter (i.e., its value never decreases over time), then the is_counter key corresponding to this metric is set to True; otherwise, it is set to False.\n  If the value of a metric is unavailable (for example, Prometheus returned NaN or a null value for the query corresponding to his metric), then, by default, iter8 sets the value of this metric to 0. This can be changed to any other float value (specified by the user in string format, e.g., \u0026quot;22.8\u0026quot;) or to \u0026quot;None\u0026quot; using the absent_value key.\n  Finally, each metric is associated with another key sample_size_query_template whose value is a Prometheus query template. Iter8 relies on the notion of a sample-size to compute the total number of data points used in the computation of the metric values. Each of the iter8-defined metrics is associated with the iter8_sample_size query template defined under query_templates, which computes the total number of requests received by a service version. For the default iter8 metrics (mean latency, error count, and error rate), the total number of requests is the correct sample size measure.\n  Adding a new metric Next, we describe how iter8 can be extended with new metrics through the iter8-metrics ConfigMap. Any metric you define in the ConfigMap can then be referenced in the success criteria of experiments.\nAs an example, we will define a new metric called error_count_400s which computes the total count of HTTP requests that resulted in a 400 HTTP status code. Adding a new metric involves creating a Prometheus query template and associating this template with the metric definition. We now describe the structure of a Prometheus query template.\nPrometheus query template A sample query template is shown below:\nsum(increase(istio_requests_total{response_code=~'4..',reporter='source'}[$interval]$offset_str)) by ($entity_labels) As shown above, the query template has three placeholders (i.e., terms beginning with $). These placeholders are substituted with actual values by iter8-analytics in order to construct a Prometheus query. 1) The query template has a group by clause (specified using the by keyword) with the placeholder $entity_labels as the group key. Each group in a Prometheus response corresponds to a distinct entity. Iter8-analytics maps service versions to Prometheus entities using this placeholder. 2) The time period of aggregation is captured by the placeholder $interval. 3) The placeholder $offset_str is used by iter8-analytics to deal with historical data when available. All three placeholders are required in the query template. When a template is instantiated (i.e., placeholders are substituted with values), it results in a Prometheus query expression; when we query Prometheus using this expression, the response from Prometheus needs to be an instant vector.\nUpdating the iter8-metrics ConfigMap There are two steps involved in adding a new metric. Step 1: Extend the query_templates section of the ConfigMap.\nerror_count_400s: \u0026#34;sum(increase(istio_requests_total{job=\u0026#39;istio-mesh\u0026#39;,response_code=~\u0026#39;4..\u0026#39;,reporter=\u0026#39;source\u0026#39;}[$interval]$offset_str)) by ($entity_labels)\u0026#34; For example, we have added our sample query template under a new key called error_count_400s in the query_templates section of the ConfigMap. This query assumes that we are using Istio telemetry v1.\nStep 2: We define a new metric in the metrics section of the ConfigMap as follows:\n- name: error_count_400s is_counter: True sample_size_query_template: iter8_sample_size The interpretation of the above definition is as follows.\n  name: Name of the new metric being defined. Its value is the key for the associated query template. In our example, the relevant key is error_count_400s.\n  is_counter: Set this to True if this metric is a counter metric. In our example, 4xx errors can never decrease over time, and therefore is_counter is set to True.\n  sample_size_query_template: As explained earlier, this is the sample size query template associated with this metric. The sample over which this value is computed for a version is the set of all HTTP requests received by the version. Hence, we are relying on the pre-defined sample-size query template iter8_sample_size, which computes the total number of HTTP requests for a version. If you are defining a metric that requires the sample size to be computed differently, you can create a new sample-size query template (with its own unique name) in the query_templates section and reference it in the metric declaration.\n  "},{"uri":"https://alan-cha.github.io/hugo-iter8-docs/reference/","title":"Reference","tags":[],"description":"","content":"Reference  Iter8\u0026#39;s metrics  Iter8\u0026rsquo;s out-of-the-box metrics and how users can define their own metrics\n   Iter8\u0026#39;s experiment CRD  A new Kubernetes CRD that compares baseline and candidate deployments\n   Iter8\u0026#39;s algorithms  The algorithms used to make decisions during canary releases or A/B testing\n   "},{"uri":"https://alan-cha.github.io/hugo-iter8-docs/tutorials/","title":"Tutorials","tags":[],"description":"","content":"Tutorials  Getting started with canary testing  Learn how to perform a canary release\n   "},{"uri":"https://alan-cha.github.io/hugo-iter8-docs/introduction/installation/","title":"Installation","tags":[],"description":"","content":"Installation  Iter8 on Kubernetes and Istio  Learn how to install iter8 on Kubernetes and Istio\n   Iter8 on Red Hat OpenShift  Learn how to install iter8 on Red Hat OpenShift\n   "},{"uri":"https://alan-cha.github.io/hugo-iter8-docs/reference/experiment/","title":"Iter8&#39;s experiment CRD","tags":[],"description":"","content":"When iter8 is installed, a new Kubernetes CRD is added to your cluster. Our CRD kind and current API version are as follows:\napiVersion: iter8.tools/v1alpha1 kind: Experiment Below we document iter8\u0026rsquo;s Experiment CRD. For clarity, we break the documentation down into the CRD\u0026rsquo;s 4 sections: spec, action, metrics, and status.\nExperiment spec Following the Kubernetes model, the spec section specifies the details of the object and its desired state. The spec of an Experiment custom resource identifies the target service of a candidate release or A/B test, the baseline deployment corresponding to the stable service version, the candidate deployment corresponding to the service version being assessed, etc. In the YAML representation below, we show sample values for the spec attributes and comments describing their meaning and whether or not they are optional.\nspec: # targetService specifies the reference to experiment targets targetService: # kind of baseline/candidate # options: Service, Deployment # default is Deployment kind: Deployment # name of a kubernetes service which receives actual traffic to the application # it\u0026#39;s required when baseline/candidate are specified as deployments name: reviews # hosts specifies how baseline/candidate can be accessed from outside the cluster # Each entry contains the name of a host and the gateway(istio) associated with it # This is optional and only applies to services directly accessible from outside the K8s cluster hosts: - name: reviews.com gateway: bookinfo-gateway # Name of the baseline and candidate versions (required) # If kind (above) is Deployment, baseline and candidate are references to K8s deployments # If kind (above) is Service, baseline and candidate are references to K8s services baseline: reviews-v3 candidate: reviews-v5 # port of the kubernetes service(.spec.targetService.name) that receives traffic # When there is only one port listening on the service, this is optional # If baseline/candidate are services, they should share the same port number port: 9080 # analysis contains the parameters for configuring the analytics service analysis: # analyticsService specifies analytics service endpoint (optional) # default value is http://iter8-analytics.iter8:8080 analyticsService: http://iter8-analytics.iter8:8080 # endpoint to Grafana dashboard (optional) # default is http://localhost:3000 grafanaEndpoint: http://localhost:3000 # successCriteria is a list of criteria for assessing the candidate version (optional) # if the list is empty, the controller will not rely on the analytics service successCriteria: # metricName: name of the metric to which this criterion applies (required) # the name should match the name of an iter8 metric or that of a user-defined custom metric # names of metrics supported by iter8 out of the box: # iter8_latency: mean latency of the service # iter8_error_rate: mean error rate (~5** HTTP Status codes) of the service # iter8_error_count: total error count (~5** HTTP Status codes) of the service - metricName: iter8_latency # minimum number of data points required to make a decision based on this criterion (optional) # default is 10 # Used by the check_and_increment and epsilon_greedy algorithms # Ignored by other algorithms sampleSize: 100 # the metric value for the candidate version defining this success criterion (required) # it can be an absolute threshold or one relative to the baseline version, depending on the # attribute toleranceType described next tolerance: 0.2 # indicates if the tolerance value above should be interpreted as an absolute threshold or # a threshold relative to the baseline (required) # options: # threshold: the metric value for the candidate must be below the tolerance value above # delta: the tolerance value above indicates the percentage within which the candidate metric value can deviate # from the baseline metric value toleranceType: threshold # The range of possible metric values (optional) # Used by bayesian routing algorithms if available. # Ignored by other algorithms. min_max: # The minimum possible value for the metric min: 0.0 # The maximum possible value for the metric max: 1.0 # indicates whether or not the experiment must finish if this criterion is not satisfied (optional) # default is false stopOnFailure: false # reward is an optional field that can be used when an a/b testing is conducted # When both versions satisfy all the success criteria, the one with higher reward value wins the comparison # This is effective when a bayesian routing strategy is specified in trafficControl (posterior_bayesian_routing or optimistic_bayesian_routing) reward: # the metric whose value is treated as reward (required) - metricName: iter8_latency # The range of possible metric values (optional) min_max: # The minimum possible value for the metric min: 0.0 # The maximum possible value for the metric max: 1.0 # trafficControl controls the experiment durarion and how the controller should change the traffic split trafficControl: # frequency with which the controller calls the analytics service # it corresponds to the duration of each \u0026#34;iteration\u0026#34; of the experiment interval: 30s # maximum number of iterations for this experiment (optional) # the duration of an experiment is defined by maxIterations * internal # default is 100 maxIterations: 6 # the maximum traffic percentage to send to the candidate during an experiment (optional) # default is 50 maxTrafficPercentage: 80 # strategy used to analyze the candidate and shift the traffic (optional) # except for the strategy increment_without_check, the analytics service is called # at each iteration and responds with the appropriate traffic split which the controller honors # options: # check_and_increment # epsilon_greedy # posterior_bayesian_routing # optimistic_bayesian_routing # increment_without_check: increase traffic to candidate by trafficStepSize at each iteration without calling analytics # default is check_and_increment strategy: check_and_increment # the maximum traffic increment per iteration (optional) # default is 2.0 # Used by check_and_increment algorithm # Ignored by other algorithms trafficStepSize: 20 # The required confidence in the recommended traffic split (optional) # default is 0.95 # Used by bayesian routing algorithms # Ignored by other algorithms confidence: 0.9 # determines how the traffic must be split at the end of the experiment (optional) # options: # baseline: all traffic goes to the baseline version # candidate: all traffic goes to the candidate version # both: traffic is split across baseline and candidate # default is candidate onSuccess: candidate # indicates whether or not iter8 should perform a clean-up action at the end of the experiment (optional) # if no action is specified, nothing is done to clean up at the end # if used, the currently supported actions are: # delete: at the end of the experiment, the version that ends up with no traffic (if any) is deleted cleanup: Experiment action: user-provided action The user can interfere with an ongoing experiment by setting the value of an action attribute. Iter8 currently supports 4 user actions: pause, resume, override_success, and override_failure. Pause and resume are self-explanatory. The action override_success causes the experiment to immediately terminate with a success status, whereas override_failure causes the experiment to terminate with a failure status.\n# user-provided input (optional) # options: # pause: pause the experiment # resume: resume a paused experiment # override_success: terminate the experiment indicating that the candidate succeeded # override_failure: abort the experiment indicating that the candidate failed action: \u0026#34;\u0026#34; Experiment metrics Information about all Prometheus metrics known to iter8 are stored in a Kubernetes ConfigMap named iter8config-metrics. When iter8 is installed, that ConfigMap is populated with information on the 3 metrics that iter8 supports out of the box, namely: iter8_latency, iter8_error_rate, and iter8_error_count. Users can add their own custom metrics.\nWhen an Experiment custom resource is created, the iter8 controller will check the metric names referenced by .spec.analysis.successCriteria, look them up in the ConfigMap, retrieve the information about them from the ConfigMap, and store that information in the metrics section of the newly created Experiment object. The information about a metric allows the iter8 analytics service to query Prometheus to retrieve metric values for the baseline version and candidate versions of the service . Below we show an example of how a metric is stored in an Experiment object.\nmetrics: iter8_latency: absent_value: None is_counter: false query_template: (sum(increase(istio_request_duration_seconds_sum{job=\u0026#39;istio-mesh\u0026#39;,reporter=\u0026#39;source\u0026#39;}[$interval]$offset_str)) by ($entity_labels)) / (sum(increase(istio_request_duration_seconds_count{job=\u0026#39;istio-mesh\u0026#39;,reporter=\u0026#39;source\u0026#39;}[$interval]$offset_str)) by ($entity_labels)) sample_size_template: sum(increase(istio_requests_total{job=\u0026#39;istio-mesh\u0026#39;,reporter=\u0026#39;source\u0026#39;}[$interval]$offset_str)) by ($entity_labels) Experiment status Following the Kubernetes model, the status section contains all relevant runtime details pertaining to the Experiment custom resource. In the YAML representation below, we show sample values for the status attributes and comments describing their meaning.\nstatus: # the last analysis state analysisState: {} # assessment returned from the analytics service assessment: conclusions: - The experiment needs to be aborted - All success criteria were not met # list of boolean conditions describing the status of the experiment # for each condition, if the status is \u0026#34;False\u0026#34;, the reason field will give detailed explanations # lastTransitionTime records the time when the last change happened to the corresponding condition # when a condition is not set, its status will be \u0026#34;Unknown\u0026#34; conditions: # AnalyticsServiceNormal is \u0026#34;True\u0026#34; when the controller can get an interpretable response from the analytics service - lastTransitionTime: \u0026#34;2019-12-20T05:38:37Z\u0026#34; status: \u0026#34;True\u0026#34; type: AnalyticsServiceNormal # ExperimentCompleted tells whether the experiment is completed or not - lastTransitionTime: \u0026#34;2019-12-20T05:39:37Z\u0026#34; status: \u0026#34;True\u0026#34; type: ExperimentCompleted # ExperimentSucceeded indicates whether the experiment succeeded or not when it is completed - lastTransitionTime: \u0026#34;2019-12-20T05:39:37Z\u0026#34; message: Aborted reason: ExperimentFailed status: \u0026#34;False\u0026#34; type: ExperimentSucceeded # MetricsSynced states whether the referenced metrics have been retrieved from the ConfigMap and stored in the metrics section - lastTransitionTime: \u0026#34;2019-12-20T05:38:22Z\u0026#34; status: \u0026#34;True\u0026#34; type: MetricsSynced # Ready records the status of the latest-updated condition - lastTransitionTime: \u0026#34;2019-12-20T05:39:37Z\u0026#34; message: Aborted reason: ExperimentFailed status: \u0026#34;False\u0026#34; type: Ready # RoutingRulesReady indicates whether the routing rules are successfully created/updated - lastTransitionTime: \u0026#34;2019-12-20T05:38:22Z\u0026#34; tatus: \u0026#34;True\u0026#34; type: RoutingRulesReady # TargetsProvided is \u0026#34;True\u0026#34; when both the baseline and the candidate versions of the targetService are detected by the controller; otherwise, missing elements will be shown in the reason field - lastTransitionTime: \u0026#34;2019-12-20T05:38:37Z\u0026#34; status: \u0026#34;True\u0026#34; type: TargetsProvided # the current experiment\u0026#39;s iteration currentIteration: 2 # Unix timestamp in nanoseconds when the experiment is created createTimestamp: 1576820317351000 # Unix timestamp in nanoseconds when the experiment started startTimestamp: 1576820317351000 # Unix timestamp in nanoseconds when the experiment finished endTimestamp: 1576820377696000 # The url to he Grafana dashboard pertaining to this experiment grafanaURL: http://localhost:3000/d/eXPEaNnZz/iter8-application-metrics?var-namespace=bookinfo-iter8\u0026amp;var-service=reviews\u0026amp;var-baseline=reviews-v3\u0026amp;var-candidate=reviews-v5\u0026amp;from=1576820317351\u0026amp;to=1576820377696 # the time when the previous iteration was completed lastIncrementTime: \u0026#34;2019-12-20T05:39:07Z\u0026#34; # this is the message to be shown in the STATUS column for the `kubectl` printer, which summarizes the experiment situation message: \u0026#39;ExperimentFailed: Aborted\u0026#39; # the experiment\u0026#39;s current phase # values could be: Progressing, Pause, Completed phase: Completed # the current traffic split trafficSplitPercentage: baseline: 100 candidate: 0 "},{"uri":"https://alan-cha.github.io/hugo-iter8-docs/reference/algorithms/","title":"Iter8&#39;s algorithms","tags":[],"description":"","content":"This documentation briefly describes the algorithms supported by iter8 to make decisions during canary releases or A/B testing. These algorithms are part of iter8\u0026rsquo;s analytics service (iter8-analytics) and exposed via REST API. Iter8\u0026rsquo;s Kubernetes controller (iter8-controller) calls the appropriate REST API based on the .spec.trafficControl.strategy set in a custom Experiment resource. Iter8\u0026rsquo;s Experiment CRD is documented here.\nIter8\u0026rsquo;s algorithms are statistically robust. Below, we list the algorithms currently available to users. This list will grow as we introduce other sophisticated algorithms for decision making.\n1. Progressive check-and-increment algorithm (check_and_increment) Input parameters interval: # (time; e.g., 30s) maxIterations: # (integer; e.g., 1000) trafficStepSize: # (percentage; e.g., 5) maxTrafficPercentage: # (percentage; e.g., 90) onSuccess: # (string enum; possible values are: \u0026#34;candidate\u0026#34;, \u0026#34;baseline\u0026#34;, \u0026#34;both\u0026#34;) This algorithm is suitable for the gradual rollout of a candidate (\u0026ldquo;canary\u0026rdquo;) version. The goal of this strategy is to gradually shift traffic from a baseline (stable) version to a candidate version, as long as the candidate version continues to pass the success criteria defined by the user.\nWhen the experiment begins, the traffic split is as follows: trafficStepSize% to the candidate version, and 100 - trafficStepSize% to the baseline version. At the end of each iteration (whose duration is determined by the interval parameter), iter8 checks if there are enough data points to decide whether the candidate version satisfies the success criteria (i.e, whether enough requests were sent to make a statistically robust assessment). If there is enough data to make a decision, and the candidate version satisfies all criteria, iter8 increases the traffic to the candidate version by trafficStepSize. Else, if there is insufficient data, or if the candidate version fails to satisfy one or more success criteria, then the traffic split does not change. Furthermore, if a failing criterion has been declared by the user as critical, iter8 aborts the experiment and makes sure all traffic goes to the baseline version. This is a rollback situation.\nA successful experiment will last for a duration of length interval * maxIterations. In case of success, the user can specify whether iter8 should: (1) send all traffic to the candidate; (2) roll back to the baseline despite success; or (3) split traffic across both versions. If the traffic is to be split across both versions, then the final split will be as follows: maxTrafficPercentage% to the candidate and 1 - maxTrafficPercentage to the baseline.\n2. Decaying epsilon-greedy algorithm (epsilon_greedy) interval: # (time; e.g., 30s) maxIterations: # (integer; e.g., 1000) maxTrafficPercentage: # (percentage; e.g., 90) onSuccess: # (string enum; possible values are: \u0026#34;candidate\u0026#34;, \u0026#34;baseline\u0026#34;, \u0026#34;both\u0026#34;) This algorithm can be applied to canary releases as well as A/B or A/B/n testing. The goal of this strategy is to explore two or more competing versions, aiming to maximize a reward (which is typically associated with business-oriented metrics) while making sure that the defined success criteria (typically associated with performance-oriented and/or correctness-oriented metrics) are satisfied.\nUnlike the check-and-increment strategy described above, this algorithm automatically decides what the proper traffic split should be at the end of each iteration and does not require the user to supply a value for the traffic increment per iteration. It converges relatively quickly to the \u0026ldquo;optimal\u0026rdquo; version as more iterations occur over time.\nIn A/B or A/B/n testing, the \u0026ldquo;optimality\u0026rdquo; of a version relates to maximizing the reward during the course of an experiment while satisfying the success criteria. In the context of canary releases, an implicit reward metric is used to indicate whether or not the success criteria are satisfied at each iteration.\n3. Posterior Bayesian Routing (PBR) (posterior_bayesian_routing) interval: # (time; e.g., 30s) maxIterations: # (integer; e.g., 1000) maxTrafficPercentage: # (percentage; e.g., 90) confidence: # (float; e.g, 0.95) onSuccess: # (string enum; possible values are: \u0026#34;candidate\u0026#34;, \u0026#34;baseline\u0026#34;, \u0026#34;both\u0026#34;) This algorithm provides a robust way of shifting application traffic to the best version using the principles of Bayesian statistics. Like the decaying epsilon-greedy strategy described above, PBR can be applied to canary releases as well as A/B or A/B/n testing scenarios. The goal of this strategy is to shift traffic to the optimal version subject to the user-defined success criteria.\nIn this algorithm, the metrics used in success criteria are associated with Bayesian belief distributions, specifically, whose parameters are learnt from metric observations through the course of the experiment. At each iteration, the algorithm computes the probability of the candidate being the \u0026ldquo;best\u0026rdquo; version (i.e., satisfying all the success criteria) and the probability of the baseline being the \u0026ldquo;best\u0026rdquo; version (i.e, the complementary probability to what is computed above). Traffic is split across these two versions in proportion to their probabilities. At the end of the experiment, if the probability of candidate being the \u0026ldquo;best\u0026rdquo; version exceeds the value of confidence parameter, then, the experiment is declared a success and a roll-forward to candidate occurs.\n4. Optimistic Bayesian Routing (OBR) (optimistic_bayesian_routing) interval: # (time; e.g., 30s) maxIterations: # (integer; e.g., 1000) maxTrafficPercentage: # 80 (percentage; e.g., 90) confidence: # (float; e.g, 0.95) onSuccess: # (string enum; possible values are: \u0026#34;candidate\u0026#34;, \u0026#34;baseline\u0026#34;, \u0026#34;both\u0026#34;) Optimistic Bayesian Routing is a variation of PBR, sharing the same goal of shifting traffic to the optimal version in a statistically robust manner. The main difference between OBR and PBR lies in the way values are sampled from the distributions for reward and feasibility constraints: this algorithm has a more optimistic approach and tends to exhibit a faster convergence rate.\n"},{"uri":"https://alan-cha.github.io/hugo-iter8-docs/releases/","title":"Older releases","tags":[],"description":"","content":" v0.2.1 v0.2.0 v0.1.1 v0.1.0 v0.0.1  "},{"uri":"https://alan-cha.github.io/hugo-iter8-docs/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"https://alan-cha.github.io/hugo-iter8-docs/tags/","title":"Tags","tags":[],"description":"","content":""}]